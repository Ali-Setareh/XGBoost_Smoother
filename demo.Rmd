---
title: "R Notebook"
output: html_notebook
---

# example 1: Pension Data 

```{r}
library(hdm)
library(grf)
library(xgboost)
library(caret)
library(glue)
library(tidyverse)
source("xgboost_smoother_2.R")

Y = pension$net_tfa
#treatment
D = pension$p401
# Create main effects matrix
X = model.matrix(~ 0 + age + db + educ + fsize + hown + inc + male + marr + pira + twoearn, data = pension)
```

```{r}
library(caret)

set.seed(123)

train_index = createDataPartition(pension$net_tfa, p = 0.7, list = FALSE)

pension_train = pension[train_index, ]
pension_test = pension[-train_index, ]

Y_train = pension_train$net_tfa
D_train = pension_train$p401
X_train = model.matrix(~ 0 + age + db + educ + fsize + hown + inc + male + marr + pira + twoearn, data = pension_train)

Y_test = pension_test$net_tfa
D_test = pension_test$p401
X_test = model.matrix(~ 0 + age + db + educ + fsize + hown + inc + male + marr + pira + twoearn, data = pension_test)

```



```{r}
# Convert to DMatrix object
dtrain = xgb.DMatrix(data = as.matrix(X_train), label = Y_train)
dtest = xgb.DMatrix(data = as.matrix(X_test), label = Y_test)

glue("train dimensions: {dim(dtrain)[1]} x {dim(dtrain)[2]}")
glue("test dimensions: {dim(dtest)[1]} x {dim(dtest)[2]}")
# Define model parameters
params = list(
  booster = "gbtree",
  objective = "reg:squarederror",
  eta = 0.85, # Equivalent to learning_rate
  max_depth = 6, # Need to specify a value as XGBoost requires a numerical value
  min_child_weight = 1, # Not a direct equivalent but serves to control over-fitting
  subsample = 1,
  colsample_bytree = 1, # Equivalent to 'sqrt' in max_features
  # Note: XGBoost does not have a direct equivalent for 'max_leaf_nodes' and 'init'
  lambda = 1,
  base_score = 0.0
)


# Number of boosting rounds (equivalent to n_estimators)
nrounds = 10

# Train the model
model = xgb.train(params = params, data = dtrain, nrounds = nrounds)
```
```{r}
help(xgboost)
```

```{r}
params = list(
  booster = "gbtree",
  objective = "reg:squarederror",
  eta = 0.03096498, # Equivalent to learning_rate
  max_depth = 6, # Need to specify a value as XGBoost requires a numerical value
  min_child_weight = 1, # Not a direct equivalent but serves to control over-fitting
  subsample = 1,
  colsample_bytree = 0.3393109, # Equivalent to 'sqrt' in max_features
  # Note: XGBoost does not have a direct equivalent for 'max_leaf_nodes' and 'init'
  lambda = 5.04033
  #base_score = 0.0
)
```


```{r}

#leaf_indices_train = predict(model, dtrain, predleaf = TRUE)
#leaf_indices_test = predict(model, dtest, predleaf = TRUE)

#smoothers = create_S_from_gbtregressor(model,leaf_indices_train,leaf_indices_test)

smoothers = get_xgboost_weights(model,dtrain,dtest)
```

```{r}
# plot the first tree
xgb.plot.tree(model = model, trees = 1,plot_width = 1000, 
                           plot_height = 1000)
```



```{r}
smoother_train = smoothers$S_train
smoother_test = smoothers$S_test


xgb_pred_train = predict(model, dtrain)
xgb_pred_test = predict(model, dtest)

smoother_pred_train = smoother_train%*% Y_train
smoother_pred_test = smoother_test%*% Y_train

```

```{r}
plot(xgb_pred_train, smoother_pred_train, col = "blue", pch = 16,
     main = "Train vs Test Predictions", xlab = "XGB Predictions", ylab = "Smoother Predictions")


points(xgb_pred_test, smoother_pred_test, col = "orange", pch = 17)


legend("bottomright", legend = c("Train", "Test"), col = c("blue", "orange"), pch = c(16, 17))


abline(a = 0, b = 1, col = "red", lty = 2) # reference line for equivalence


```
# exampe 2: Sin Function 

```{r}
y_function_smooth = function(x,b){
  y = sin(x%*%b)  #+ cos(-x%*%b)
}
```

```{r}
set.seed(123)
p = 1
j = 1
b = rep(1,p)/((1:p)^j)
X = matrix(seq(-5,5,0.01),ncol = 1)
Y = y_function_smooth(X,b) + rnorm(dim(X)[1],sd = 0.5)
y_true = y_function_smooth(X,b)
data <- tibble(X = as.numeric(X), Y = as.numeric(Y), y_true = as.numeric(y_true))

ggplot(data, aes(x = X, y = Y)) +
  geom_point(alpha = 0.2) +
  geom_line(aes(y = y_true), color = "red") +
  theme_minimal()
```
```{r}
df = data.frame(X = X, Y = Y)

set.seed(123)

train_index = createDataPartition(Y, p = 0.7, list = FALSE)

pension_train = df[train_index, ]
pension_test = df[-train_index, ]

Y_train = pension_train$Y
X_train = pension_train$X

Y_test = pension_test$Y
X_test = pension_test$X
```

```{r}
# Convert to DMatrix object
dtrain = xgb.DMatrix(data = as.matrix(X_train), label = Y_train)
dtest = xgb.DMatrix(data = as.matrix(X_test), label = Y_test)

glue("train dimensions: {dim(dtrain)[1]} x {dim(dtrain)[2]}")
glue("test dimensions: {dim(dtest)[1]} x {dim(dtest)[2]}")
# Define model parameters
params = list(
  booster = "gbtree",
  objective = "reg:squarederror",
  eta = 0.85, # Equivalent to learning_rate
  max_depth = 6, # Need to specify a value as XGBoost requires a numerical value
  min_child_weight = 1, # Not a direct equivalent but serves to control over-fitting
  subsample = 1,
  colsample_bytree = 1, # Equivalent to 'sqrt' in max_features
  # Note: XGBoost does not have a direct equivalent for 'max_leaf_nodes' and 'init'
  lambda = 1,
  base_score = 0.0,
  colsample_bynode = 0.5,
  colsample_bylevel = 1
)

# Number of boosting rounds (equivalent to n_estimators)
nrounds = 10

# Train the model
model = xgb.train(params = params, data = dtrain, nrounds = nrounds)
```
```{r}
smoothers = get_xgboost_weights(model,dtrain,dtest)
```
```{r}
smoother_train = smoothers$S_train
smoother_test = smoothers$S_test


xgb_pred_train = predict(model, dtrain)
xgb_pred_test = predict(model, dtest)

smoother_pred_train = smoother_train%*% Y_train
smoother_pred_test = smoother_test%*% Y_train

```

```{r}
plot(xgb_pred_train, smoother_pred_train, col = "blue", pch = 16,
     main = "Train vs Test Predictions", xlab = "XGB Predictions", ylab = "Smoother Predictions")


points(xgb_pred_test, smoother_pred_test, col = "orange", pch = 17)


legend("bottomright", legend = c("Train", "Test"), col = c("blue", "orange"), pch = c(16, 17))


abline(a = 0, b = 1, col = "red", lty = 2) # reference line for equivalence

```
```{r}
all.equal(as.numeric(smoother_pred_train),as.numeric(xgb_pred_train))
all.equal(as.numeric(smoother_pred_test),as.numeric(xgb_pred_test))
```


```{r}
set.seed(123)

# Create data partition indices (70% training data)
train_indices <- createDataPartition(Y, p = 0.7, list = FALSE)

# Split Y into training and testing sets
Y_train <- Y[train_indices]
Y_test <- Y[-train_indices]

# Split D into training and testing sets
D_train <- D[train_indices]
D_test <- D[-train_indices]

# Split X into training and testing sets
X_train <- X[train_indices, ]
X_test <- X[-train_indices, ]
```

```{r}
# Convert to DMatrix object
dtrain = xgb.DMatrix(data = as.matrix(X_train), label = Y_train)
dtest = xgb.DMatrix(data = as.matrix(X_test), label = Y_test)

# Define model parameters
params = list(
  booster = "gbtree",
  objective = "reg:squarederror",
  eta = 0.85, # Equivalent to learning_rate
  max_depth = 6, # Need to specify a value as XGBoost requires a numerical value
  min_child_weight = 1, # Not a direct equivalent but serves to control over-fitting
  subsample = 1,
  colsample_bytree = 1, # Equivalent to 'sqrt' in max_features
  # Note: XGBoost does not have a direct equivalent for 'max_leaf_nodes' and 'init'
  lambda = 1,
  base_score = 0.0
)

# Number of boosting rounds (equivalent to n_estimators)
nrounds = 10

# Train the model
model = xgb.train(params = params, data = dtrain, nrounds = nrounds)
```

```{r}
source("xgboost_smoother.R")
leaf_indices_train = predict(model, dtrain, predleaf = TRUE)
leaf_indices_test = predict(model, dtest, predleaf = TRUE)
smoother_test = create_S_from_gbtregressor(model,leaf_indices_train,tree_indices_test = leaf_indices_test)
```
```{r}
dim(dtrain)
```
```{r}
dim(leaf_indices_test)
```

```{r}
xgb_pred = predict(model, dtest)
smoother_pred = smoother_test%*% Y_train
plot(xgb_pred, smoother_pred)


# Show that the (out-of-bag- predictions in this case) are numerically equivalent to the smoother created predictions.
#points(predict(rfY)$predictions,SY %*% Y, col = "blue")

abline(a = 0, b = 1, col = "red")
```
```{r}
dim(smoother_train)
```

# compare with random forest

```{r}
# Outcome random forest
rfY = regression_forest(X,Y)
# Extract Smoother matrix
SY = get_forest_weights(rfY)

forest_pred = predict(rfY)$predictions
smoother_pred_for = SY %*% Y
```

```{r}
plot(forest_pred,smoother_pred_for)
```

```{r}
S_metrics_train = compute_metrics_from_S(smoother_train,Y)


names(S_metrics_train) = c("MSE", "Accuracy", "Effective Trace", "L2 Norm", "Squared L2 Norm")

# Print the results in a table format
print(S_metrics_train)
```

```{r}
metrics_train = compute_metrics_from_xgboost(model,dtrain,Y)
names(metrics_train) = c("MSE", "Accuracy")

# Print the results in a table format
print(metrics_train)
```

# Prediction_reconstruction

We can reconstruct the predictions iteratively with reconstruct_predictions function from xgboost_smoother:

```{r}
predict(model, dtrain) - reconstruct_predictions(model,dtrain)
```

As it is seen, the predictions are all most equal up to the numerical errors.

# XGBoost loss function

To make sure that the internal mean squared loss function of XGBoost is in fact $\frac{1}{2}(y-y_{pred})^2$, we use the capability of XGBOost for defining custom loss functions. So we define our loss function as $\frac{1}{2}(y-y_{pred})^2$ with gradiant and hessian as given below and then compare the predictions using our custom loss with internal XGBoost loss

```{r}
# Custom objective function for regression
custom_loss = function(preds, dtrain) {
  labels = getinfo(dtrain, "label")
  residuals = (preds - labels)
  grad = residuals  # Gradient
  hess = rep(1, length(residuals))  # Hessian
  return(list(grad = grad, hess = hess))
}
```

```{r}

params_custom <- list(
  booster = "gbtree",
  objective = custom_loss,
  eta = 0.85, # Equivalent to learning_rate
  max_depth = 6, # Need to specify a value as XGBoost requires a numerical value
  min_child_weight = 1, # Not a direct equivalent but serves to control over-fitting
  subsample = 1,
  colsample_bytree = 1, # Equivalent to 'sqrt' in max_features
  # Note: XGBoost does not have a direct equivalent for 'max_leaf_nodes' and 'init'
  lambda = 1,
  base_score = 0.0
)
model_custom = xgb.train(params_custom, dtrain, nrounds = nrounds)
```

```{r}
xgb_pred = predict(model, dtrain)
xgb_pred_custom = predict(model_custom, dtrain)

plot(xgb_pred, xgb_pred_custom)


# Show that the (out-of-bag- predictions in this case) are numerically equivalent to the smoother created predictions.
#points(predict(rfY)$predictions,SY %*% Y, col = "blue")

abline(a = 0, b = 1, col = "red")
```

As we see, the predictions resulted from two loss functions match up.

# Testing the compact version

```{r}
library(hdm)
library(grf)
library(xgboost)
source("xgboost_smoother.R")

Y = pension$net_tfa[1:1000]
# Treatment
D = pension$p401[1:1000]
# Create main effects matrix
X = model.matrix(~ 0 + age + db + educ + fsize + hown + inc + male + marr + pira + twoearn, data = pension[1:1000,])
```

```{r}
# Convert to DMatrix object
dtrain = xgb.DMatrix(data = as.matrix(X), label = Y)
#dtest = xgb.DMatrix(data = as.matrix(test_features), label = test_targets)

# Define model parameters
params = list(
  booster = "gbtree",
  objective = "reg:squarederror",
  eta = 0.85, # Equivalent to learning_rate
  max_depth = 6, # Need to specify a value as XGBoost requires a numerical value
  min_child_weight = 1, # Not a direct equivalent but serves to control over-fitting
  subsample = 1,
  colsample_bytree = 1, # Equivalent to 'sqrt' in max_features
  # Note: XGBoost does not have a direct equivalent for 'max_leaf_nodes' and 'init'
  lambda = 1,
  base_score = 0.0
)

# Number of boosting rounds (equivalent to n_estimators)
nrounds = 10

# Train the model
model = xgb.train(params = params, data = dtrain, nrounds = nrounds)
```

```{r}
leaf_indices_train = predict(model, dtrain, predleaf = TRUE)
smoother_train = create_S_from_gbtregressor(model,leaf_indices_train,output_dir,save_output = FALSE,compact = TRUE)
```

```{r}
xgb_pred = predict(model, dtrain)
smoother_pred = smoother_train%*% Y
plot(xgb_pred, smoother_pred)


# Show that the (out-of-bag- predictions in this case) are numerically equivalent to the smoother created predictions.
#points(predict(rfY)$predictions,SY %*% Y, col = "blue")

abline(a = 0, b = 1, col = "red")
```

We see that the compact version works.

```{r}
dim(dtrain_matrix)
```


```{r}
# Load necessary library
library(xgboost)
source("xgboost_smoother_3.R")

set.seed(123)
p = 1
j = 1
b = rep(1,p)/((1:p)^j)
X = matrix(seq(-5,5,0.01),ncol = 1)
Y = y_function_smooth(X,b) + rnorm(dim(X)[1],sd = 0.5)
y_true = y_function_smooth(X,b)
data <- tibble(X = as.numeric(X), Y = as.numeric(Y), y_true = as.numeric(y_true))

df = data.frame(X = X, Y = Y)

set.seed(123)

train_index = createDataPartition(Y, p = 0.7, list = FALSE)

pension_train = df[train_index, ]
pension_test = df[-train_index, ]

Y_train = pension_train$Y
X_train = pension_train$X

Y_test = pension_test$Y
X_test = pension_test$X

dtrain = xgb.DMatrix(data = as.matrix(X_train), label = Y_train)
dtest = xgb.DMatrix(data = as.matrix(X_test), label = Y_test)

subsample_rate <- 0.8
n_samples <- nrow(dtrain)

# Create the custom objective closure
custom_obj <- create_custom_objective(subsample_rate, n_samples)
custom_objective <- custom_obj$objective

# Train the model
model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 10,
  obj = custom_objective,
  verbose = 0,
  lambda = 0
)



# Retrieve the sampled indices
sampled_indices_per_tree <- custom_obj$get_indices()



```




```{r}
smoothers = get_xgboost_weights_with_subsampling(model,dtrain,dtest,sampled_indices_per_tree)
```
```{r}

smoother_train = smoothers$S_train
smoother_test = smoothers$S_test


xgb_pred_train = predict(model, dtrain)
xgb_pred_test = predict(model, dtest)

smoother_pred_train = smoother_train%*% Y_train
smoother_pred_test = smoother_test%*% Y_train

```


```{r}
plot(xgb_pred_train, smoother_pred_train, col = "blue", pch = 16,
     main = "Train vs Test Predictions", xlab = "XGB Predictions", ylab = "Smoother Predictions")


points(xgb_pred_test, smoother_pred_test, col = "orange", pch = 17)


legend("bottomright", legend = c("Train", "Test"), col = c("blue", "orange"), pch = c(16, 17))


abline(a = 0, b = 1, col = "red", lty = 2) # reference line for equivalence

```

```{r}
gradient_env$gradients[[i]][gradient_env$gradients[[i]] == 0]
```

