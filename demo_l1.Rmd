---
title: "R Notebook"
output: html_notebook
---



```{r}
# Load necessary library
library(hdm)
library(grf)
library(xgboost)
library(caret)
library(glue)
library(tidyverse)
source("xgboost_smoother_l1.R")

# Set seed for reproducibility
set.seed(123)


# ------------------------------
# Step 1: Generate Synthetic Data
# ------------------------------

y_function_smooth <- function(x, b) {
  y <- sin(x %*% b)  # + cos(-x %*% b)  # Uncomment if you want to include the cosine term
  return(y)
}



p = 5
j = 1
b = rep(1,p)/((1:p)^j)

n = 1000  # Number of training samples

# Generate random data and create the matrix
X = matrix(runif(n * p, min = -5, max = 5), nrow = n, ncol = p)
Y = y_function_smooth(X,b) + rnorm(dim(X)[1],sd = 0.5)
y_true = y_function_smooth(X,b)
df = data.frame(X = X, Y = Y)

# Split the data into training and test sets
train_index = createDataPartition(Y, p = 0.5, list = FALSE)

df_train = df[train_index, ]
df_test = df[-train_index, ]


X_train <- as.matrix(df_train[, 1:p])
X_test <- as.matrix(df_test[, 1:p])


Y_train = df_train$Y
Y_test = df_test$Y

dtrain = xgb.DMatrix(data = X_train, label = Y_train)
dtest = xgb.DMatrix(data = X_test, label = Y_test)



```

```{r}
params = list(
  booster = "gbtree",
  objective = "reg:squarederror",
  eta = 0.85, # Equivalent to learning_rate
  max_depth = 6, # Need to specify a value as XGBoost requires a numerical value
  min_child_weight = 1, # Not a direct equivalent but serves to control over-fitting
  subsample = 1,
  colsample_bytree = 1, # Equivalent to 'sqrt' in max_features
  # Note: XGBoost does not have a direct equivalent for 'max_leaf_nodes' and 'init'
  lambda = 1,
  alpha = 1,
  base_score = 0.0
)

model = xgboost(data = dtrain, params = params, nrounds = 20,verbose = 0)

# Get smoother matrices
smoothers = get_xgboost_weights_L1(model, dtrain, dtest)

S_train = smoothers$S_train
S_test = smoothers$S_test
```

```{r}
smoother_train = smoothers$S_train
smoother_test = smoothers$S_test


xgb_pred_train = predict(model, dtrain)
xgb_pred_test = predict(model, dtest)

smoother_pred_train = smoother_train%*% Y_train
smoother_pred_test = smoother_test%*% Y_train

```


```{r}
zero_rows <- apply(smoother_train, 1, function(row) all(row == 0))

# Get the indices of rows that are zeros
s_zero_indices <- which(zero_rows)

# Print the indices
print(s_zero_indices)
```

```{r}
trees = xgb.model.dt.tree(model = model)[Feature == "Leaf"]


filtered_dt <- trees[trees$Quality == 0, ]

filtered_dt

trees[Tree == 0 & Feature == "Leaf",.(Node,Quality)]

trees[Tree == 0 & Feature == "Leaf",.(Node,Quality)][Node==3]$Quality

sign(trees[Tree == 0 & Feature == "Leaf",.(Node,Quality)][Node==28]$Quality)
```
```{r}
trees = xgb.model.dt.tree(model = model)[Feature == "Leaf"]
filtered_dt <- trees[trees$Quality == 0, ]
# Assuming `leaf_inx` is the prediction result and `filtered_dt` is your filtered data frame
leaf_inx <- predict(model, dtrain, predleaf = TRUE)

# Create an empty list
l <- list()

# Loop through each value in `filtered_dt$Node`
for (i in filtered_dt$Node) {
  # Append the indices where leaf_inx matches the current node value
  l <- c(l, which(leaf_inx == i))
}

flattened_l <- unlist(l)
# Find differences
only_in_new_list <- setdiff(s_zero_indices, flattened_l)  # Elements in `new_list` but not in `flattened_l`
only_in_l <- setdiff(flattened_l, s_zero_indices)        # Elements in `flattened_l` but not in `new_list`

# Print results
if (length(only_in_new_list) == 0 && length(only_in_l) == 0) {
  print("All elements match between the new list and l.")
} else {
  print("Differences found:")
  if (length(only_in_new_list) > 0) {
    print("Elements only in the s_zero_indices:")
    print(only_in_new_list)
  }
  if (length(only_in_l) > 0) {
    print("Elements only in l:")
    print(only_in_l)
  }
}


```



```{r}
plot(xgb_pred_train, smoother_pred_train, col = "blue", pch = 16,
     main = "Train vs Test Predictions", xlab = "XGB Predictions", ylab = "Smoother Predictions")


points(xgb_pred_test, smoother_pred_test, col = "orange", pch = 17)


legend("bottomright", legend = c("Train", "Test"), col = c("blue", "orange"), pch = c(16, 17))


abline(a = 0, b = 1, col = "red", lty = 2) # reference line for equivalence

```

```{r}
all.equal(as.numeric(smoother_pred_train),as.numeric(xgb_pred_train))
all.equal(as.numeric(smoother_pred_test),as.numeric(xgb_pred_test))

```


```{r}
xgb.plot.tree(model = model,trees = 0)
```




```{r}
predict(model,dtrain,iterationrange = c(1,3))
```

```{r}
predict(model, dtrain,iterationrange = c(1,2)) + predict(model, dtrain,iterationrange = c(2,3))*model$params$eta
```

